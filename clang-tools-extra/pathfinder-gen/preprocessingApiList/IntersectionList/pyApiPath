Path: torch
  BFloat16Storage
  BoolStorage
  ByteStorage
  CharStorage
  ComplexDoubleStorage
  ComplexFloatStorage
  DoubleStorage
  FloatStorage
  Generator
  HalfStorage
  IntStorage
  LongStorage
  QInt32Storage
  QInt8Storage
  QUInt4x2Storage
  QUInt8Storage
  ShortStorage
  _assert
  abs
  absolute
  acos
  acosh
  add
  addbmm
  addcdiv
  addcmul
  addmm
  addmv
  addr
  all
  allclose
  amax
  amin
  aminmax
  angle
  any
  arange
  arccos
  arccosh
  arcsin
  arcsinh
  arctan
  arctanh
  are_deterministic_algorithms_enabled
  argmax
  argmin
  argsort
  as_strided
  as_tensor
  asin
  asinh
  atan
  atan2
  atanh
  atleast_1d
  atleast_2d
  atleast_3d
  baddbmm
  bartlett_window
  bernoulli
  bincount
  bitwise_and
  bitwise_left_shift
  bitwise_not
  bitwise_or
  bitwise_right_shift
  bitwise_xor
  blackman_window
  block_diag
  bmm
  broadcast_shapes
  broadcast_tensors
  broadcast_to
  bucketize
  can_cast
  cartesian_prod
  cat
  cdist
  ceil
  chain_matmul
  cholesky
  cholesky_inverse
  cholesky_solve
  chunk
  clamp
  clip
  clone
  column_stack
  combinations
  compiled_with_cxx11_abi
  complex
  concat
  conj
  conj_physical
  copysign
  corrcoef
  cos
  cosh
  count_nonzero
  cov
  cross
  cummax
  cummin
  cumprod
  cumsum
  cumulative_trapezoid
  deg2rad
  dequantize
  det
  diag
  diag_embed
  diagflat
  diagonal
  diff
  digamma
  dist
  div
  divide
  dot
  dsplit
  dstack
  eig
  einsum
  empty
  empty_like
  empty_strided
  enable_grad
  eq
  equal
  erf
  erfc
  erfinv
  exp
  exp2
  expm1
  eye
  fake_quantize_per_channel_affine
  fake_quantize_per_tensor_affine
  fix
  flatten
  flip
  fliplr
  flipud
  float_power
  floor
  floor_divide
  fmax
  fmin
  fmod
  frac
  frexp
  from_numpy
  frombuffer
  full
  full_like
  gather
  gcd
  ge
  geqrf
  ger
  get_default_dtype
  get_num_interop_threads
  get_num_threads
  get_rng_state
  gradient
  greater
  greater_equal
  gt
  hamming_window
  hann_window
  heaviside
  histc
  histogram
  hsplit
  hspmm
  hstack
  hypot
  i0
  igamma
  igammac
  imag
  index_select
  inference_mode
  initial_seed
  inner
  inverse
  is_complex
  is_conj
  is_floating_point
  is_grad_enabled
  is_inference_mode_enabled
  is_nonzero
  is_storage
  is_tensor
  is_warn_always_enabled
  isclose
  isfinite
  isin
  isinf
  isnan
  isneginf
  isposinf
  isreal
  istft
  kaiser_window
  kron
  kthvalue
  lcm
  ldexp
  le
  lerp
  less
  less_equal
  lgamma
  linspace
  load
  lobpcg
  log
  log10
  log1p
  log2
  logaddexp
  logaddexp2
  logcumsumexp
  logdet
  logical_and
  logical_not
  logical_or
  logical_xor
  logit
  logspace
  logsumexp
  lstsq
  lt
  lu
  lu_solve
  lu_unpack
  manual_seed
  masked_select
  matmul
  matrix_exp
  matrix_power
  matrix_rank
  max
  maximum
  mean
  median
  meshgrid
  min
  minimum
  mm
  mode
  moveaxis
  movedim
  msort
  mul
  multinomial
  multiply
  mv
  mvlgamma
  nan_to_num
  nanmean
  nanmedian
  nanquantile
  nansum
  narrow
  ne
  neg
  negative
  nextafter
  no_grad
  nonzero
  norm
  normal
  not_equal
  numel
  ones
  ones_like
  orgqr
  ormqr
  outer
  pca_lowrank
  permute
  pinverse
  poisson
  polar
  polygamma
  positive
  pow
  prod
  promote_types
  qr
  quantile
  quantize_per_channel
  quantize_per_tensor
  quantized_batch_norm
  quantized_max_pool1d
  quantized_max_pool2d
  rad2deg
  rand
  rand_like
  randint
  randint_like
  randn
  randn_like
  randperm
  range
  ravel
  real
  reciprocal
  remainder
  renorm
  repeat_interleave
  reshape
  resolve_conj
  resolve_neg
  result_type
  roll
  rot90
  round
  row_stack
  rsqrt
  save
  scatter
  scatter_add
  searchsorted
  seed
  set_default_dtype
  set_default_tensor_type
  set_flush_denormal
  set_grad_enabled
  set_num_interop_threads
  set_num_threads
  set_printoptions
  set_rng_state
  set_warn_always
  sgn
  sigmoid
  sign
  signbit
  sin
  sinc
  sinh
  slogdet
  smm
  solve
  sort
  sparse_coo_tensor
  sparse_csr_tensor
  split
  sqrt
  square
  squeeze
  sspaddmm
  stack
  std
  std_mean
  stft
  sub
  subtract
  sum
  svd
  svd_lowrank
  swapaxes
  swapdims
  symeig
  t
  take
  take_along_dim
  tan
  tanh
  tensor
  tensor_split
  tensordot
  tile
  topk
  trace
  transpose
  trapezoid
  trapz
  triangular_solve
  tril
  tril_indices
  triu
  triu_indices
  true_divide
  trunc
  unbind
  unique
  unique_consecutive
  unsqueeze
  use_deterministic_algorithms
  vander
  var
  var_mean
  vdot
  view_as_complex
  view_as_real
  vsplit
  vstack
  where
  xlogy
  zeros
  zeros_like

Path: torch.Tensor
  abs
  abs_
  absolute
  absolute_
  acos
  acos_
  acosh
  acosh_
  add
  add_
  addbmm
  addbmm_
  addcdiv
  addcdiv_
  addcmul
  addcmul_
  addmm
  addmm_
  addmv
  addmv_
  addr
  addr_
  all
  allclose
  amax
  amin
  aminmax
  angle
  any
  apply_
  arccos
  arccos_
  arccosh
  arccosh_
  arcsin
  arcsin_
  arcsinh
  arcsinh_
  arctan
  arctan_
  arctanh
  arctanh_
  argmax
  argmin
  argsort
  as_strided
  as_subclass
  asin
  asin_
  asinh
  asinh_
  atan
  atan2
  atan2_
  atan_
  atanh
  atanh_
  backward
  baddbmm
  baddbmm_
  bernoulli
  bernoulli_
  bfloat16
  bincount
  bitwise_and
  bitwise_and_
  bitwise_left_shift
  bitwise_left_shift_
  bitwise_not
  bitwise_not_
  bitwise_or
  bitwise_or_
  bitwise_right_shift
  bitwise_right_shift_
  bitwise_xor
  bitwise_xor_
  bmm
  bool
  broadcast_to
  byte
  cauchy_
  ceil
  ceil_
  char
  cholesky
  cholesky_inverse
  cholesky_solve
  chunk
  clamp
  clamp_
  clip
  clip_
  clone
  conj
  conj_physical
  conj_physical_
  contiguous
  copy_
  copysign
  copysign_
  corrcoef
  cos
  cos_
  cosh
  cosh_
  count_nonzero
  cov
  cpu
  cross
  cuda
  cummax
  cummin
  cumprod
  cumprod_
  cumsum
  cumsum_
  data_ptr
  deg2rad
  dense_dim
  dequantize
  det
  detach
  detach_
  device
  diag
  diag_embed
  diagflat
  diagonal
  diff
  digamma
  digamma_
  dim
  dist
  div
  div_
  divide
  divide_
  dot
  double
  dsplit
  eig
  element_size
  eq
  eq_
  equal
  erf
  erf_
  erfc
  erfc_
  erfinv
  erfinv_
  exp
  exp_
  expand
  expand_as
  expm1
  expm1_
  exponential_
  fill_
  fill_diagonal_
  fix
  fix_
  flatten
  flip
  fliplr
  flipud
  float
  float_power
  float_power_
  floor
  floor_
  floor_divide
  floor_divide_
  fmax
  fmin
  fmod
  fmod_
  frac
  frac_
  frexp
  gather
  gcd
  gcd_
  ge
  ge_
  geometric_
  geqrf
  ger
  get_device
  grad
  greater
  greater_
  greater_equal
  greater_equal_
  gt
  gt_
  half
  hardshrink
  heaviside
  histc
  histogram
  hsplit
  hypot
  hypot_
  i0
  i0_
  igamma
  igamma_
  igammac
  igammac_
  imag
  index_add
  index_add_
  index_copy
  index_copy_
  index_fill
  index_fill_
  index_put
  index_put_
  index_select
  indices
  inner
  int
  int_repr
  inverse
  is_complex
  is_conj
  is_contiguous
  is_cuda
  is_floating_point
  is_inference
  is_leaf
  is_meta
  is_pinned
  is_quantized
  is_set_to
  is_shared
  is_signed
  is_sparse
  isclose
  isfinite
  isinf
  isnan
  isneginf
  isposinf
  isreal
  istft
  item
  kthvalue
  lcm
  lcm_
  ldexp
  ldexp_
  le
  le_
  lerp
  lerp_
  less
  less_
  less_equal
  less_equal_
  lgamma
  lgamma_
  log
  log10
  log10_
  log1p
  log1p_
  log2
  log2_
  log_
  log_normal_
  logaddexp
  logaddexp2
  logcumsumexp
  logdet
  logical_and
  logical_and_
  logical_not
  logical_not_
  logical_or
  logical_or_
  logical_xor
  logical_xor_
  logit
  logit_
  logsumexp
  long
  lstsq
  lt
  lt_
  lu
  lu_solve
  map_
  masked_fill
  masked_fill_
  masked_scatter
  masked_scatter_
  masked_select
  matmul
  matrix_exp
  matrix_power
  max
  maximum
  mean
  median
  min
  minimum
  mm
  mode
  moveaxis
  movedim
  msort
  mul
  mul_
  multinomial
  multiply
  multiply_
  mv
  mvlgamma
  mvlgamma_
  nan_to_num
  nan_to_num_
  nanmean
  nanmedian
  nanquantile
  nansum
  narrow
  narrow_copy
  ndim
  ndimension
  ne
  ne_
  neg
  neg_
  negative
  negative_
  nelement
  new_empty
  new_full
  new_ones
  new_tensor
  new_zeros
  nextafter
  nextafter_
  nonzero
  norm
  normal_
  not_equal
  not_equal_
  numel
  numpy
  orgqr
  ormqr
  outer
  permute
  pin_memory
  pinverse
  polygamma
  polygamma_
  positive
  pow
  pow_
  prod
  put_
  q_per_channel_axis
  q_per_channel_scales
  q_per_channel_zero_points
  q_scale
  q_zero_point
  qr
  qscheme
  quantile
  rad2deg
  random_
  ravel
  real
  reciprocal
  reciprocal_
  record_stream
  register_hook
  remainder
  remainder_
  renorm
  renorm_
  repeat
  repeat_interleave
  requires_grad
  requires_grad_
  reshape
  reshape_as
  resize_
  resize_as_
  resolve_conj
  resolve_neg
  retain_grad
  retains_grad
  roll
  rot90
  round
  round_
  rsqrt
  rsqrt_
  scatter
  scatter_
  scatter_add
  scatter_add_
  select
  set_
  sgn
  sgn_
  share_memory_
  short
  sigmoid
  sigmoid_
  sign
  sign_
  signbit
  sin
  sin_
  sinc
  sinc_
  sinh
  sinh_
  size
  slogdet
  smm
  solve
  sort
  sparse_dim
  sparse_mask
  split
  sqrt
  sqrt_
  square
  square_
  squeeze
  squeeze_
  sspaddmm
  std
  stft
  storage
  storage_offset
  storage_type
  stride
  sub
  sub_
  subtract
  subtract_
  sum
  sum_to_size
  svd
  swapaxes
  swapdims
  symeig
  t
  t_
  take
  take_along_dim
  tan
  tan_
  tanh
  tanh_
  tensor_split
  tile
  to
  to_mkldnn
  to_sparse
  tolist
  topk
  trace
  transpose
  transpose_
  triangular_solve
  tril
  tril_
  triu
  triu_
  true_divide
  true_divide_
  trunc
  trunc_
  type
  type_as
  unbind
  unfold
  uniform_
  unique
  unique_consecutive
  unsqueeze
  unsqueeze_
  values
  var
  vdot
  view
  view_as
  vsplit
  where
  xlogy
  xlogy_
  zero_

Path: torch.Tensor.Alias for :meth:`~Tensor
  dim

Path: torch.distributed
  Backend
  FileStore
  HashStore
  PrefixStore
  ReduceOp
  Store
  TCPStore
  all_gather
  all_gather_multigpu
  all_gather_object
  all_reduce
  all_reduce_multigpu
  all_to_all
  barrier
  broadcast
  broadcast_multigpu
  broadcast_object_list
  gather
  gather_object
  get_backend
  get_rank
  get_world_size
  init_process_group
  irecv
  is_available
  is_initialized
  is_mpi_available
  is_nccl_available
  is_torchelastic_launched
  isend
  monitored_barrier
  new_group
  recv
  reduce
  reduce_multigpu
  reduce_op
  reduce_scatter
  reduce_scatter_multigpu
  scatter
  scatter_object_list
  send

Path: torch.distributed.algorithms
  Join
  JoinHook
  Joinable

Path: torch.distributed.elastic.agent.server
  ElasticAgent
  SimpleElasticAgent
  Worker
  WorkerGroup
  WorkerSpec
  WorkerState

Path: torch.distributed.elastic.agent.server.api
  RunResult

Path: torch.distributed.elastic.agent.server.local_elastic_agent
  LocalElasticAgent

Path: torch.distributed.elastic.events
  get_logging_handler
  record

Path: torch.distributed.elastic.events.api
  Event
  EventSource

Path: torch.distributed.elastic.metrics
  configure
  prof
  put_metric

Path: torch.distributed.elastic.metrics.api
  ConsoleMetricHandler
  MetricHandler
  NullMetricHandler

Path: torch.distributed.elastic.multiprocessing
  start_processes

Path: torch.distributed.elastic.multiprocessing.api
  MultiprocessContext
  PContext
  RunProcsResult
  SubprocessContext

Path: torch.distributed.elastic.multiprocessing.errors
  ChildFailedError
  ErrorHandler
  ProcessFailure
  record

Path: torch.distributed.elastic.rendezvous
  RendezvousClosedError
  RendezvousConnectionError
  RendezvousError
  RendezvousHandler
  RendezvousHandlerRegistry
  RendezvousParameters
  RendezvousStateError
  RendezvousTimeoutError

Path: torch.distributed.elastic.rendezvous.c10d_rendezvous_backend
  C10dRendezvousBackend
  create_backend

Path: torch.distributed.elastic.rendezvous.dynamic_rendezvous
  DynamicRendezvousHandler
  RendezvousBackend
  RendezvousTimeout
  create_handler

Path: torch.distributed.elastic.rendezvous.etcd_rendezvous
  EtcdRendezvousHandler

Path: torch.distributed.elastic.rendezvous.etcd_rendezvous_backend
  EtcdRendezvousBackend
  create_backend

Path: torch.distributed.elastic.rendezvous.etcd_server
  EtcdServer

Path: torch.distributed.elastic.rendezvous.etcd_store
  EtcdStore

Path: torch.distributed.elastic.timer
  LocalTimerClient
  LocalTimerServer
  TimerClient
  TimerRequest
  TimerServer
  configure
  expires

Path: torch.distributed.optim
  ZeroRedundancyOptimizer

Path: torch.distributions.bernoulli
  Bernoulli

Path: torch.distributions.beta
  Beta

Path: torch.distributions.binomial
  Binomial

Path: torch.distributions.categorical
  Categorical

Path: torch.distributions.cauchy
  Cauchy

Path: torch.distributions.chi2
  Chi2

Path: torch.distributions.constraint_registry
  ConstraintRegistry

Path: torch.distributions.constraints
  Constraint

Path: torch.distributions.continuous_bernoulli
  ContinuousBernoulli

Path: torch.distributions.dirichlet
  Dirichlet

Path: torch.distributions.distribution
  Distribution

Path: torch.distributions.exp_family
  ExponentialFamily

Path: torch.distributions.exponential
  Exponential

Path: torch.distributions.fishersnedecor
  FisherSnedecor

Path: torch.distributions.gamma
  Gamma

Path: torch.distributions.geometric
  Geometric

Path: torch.distributions.gumbel
  Gumbel

Path: torch.distributions.half_cauchy
  HalfCauchy

Path: torch.distributions.half_normal
  HalfNormal

Path: torch.distributions.independent
  Independent

Path: torch.distributions.kl
  kl_divergence
  register_kl

Path: torch.distributions.kumaraswamy
  Kumaraswamy

Path: torch.distributions.laplace
  Laplace

Path: torch.distributions.lkj_cholesky
  LKJCholesky

Path: torch.distributions.log_normal
  LogNormal

Path: torch.distributions.lowrank_multivariate_normal
  LowRankMultivariateNormal

Path: torch.distributions.mixture_same_family
  MixtureSameFamily

Path: torch.distributions.multinomial
  Multinomial

Path: torch.distributions.multivariate_normal
  MultivariateNormal

Path: torch.distributions.negative_binomial
  NegativeBinomial

Path: torch.distributions.normal
  Normal

Path: torch.distributions.one_hot_categorical
  OneHotCategorical

Path: torch.distributions.pareto
  Pareto

Path: torch.distributions.poisson
  Poisson

Path: torch.distributions.relaxed_bernoulli
  LogitRelaxedBernoulli
  RelaxedBernoulli

Path: torch.distributions.relaxed_categorical
  RelaxedOneHotCategorical

Path: torch.distributions.studentT
  StudentT

Path: torch.distributions.transformed_distribution
  TransformedDistribution

Path: torch.distributions.transforms
  AbsTransform
  AffineTransform
  ComposeTransform
  CorrCholeskyTransform
  ExpTransform
  IndependentTransform
  LowerCholeskyTransform
  PowerTransform
  ReshapeTransform
  SigmoidTransform
  SoftmaxTransform
  StackTransform
  StickBreakingTransform
  TanhTransform
  Transform

Path: torch.distributions.uniform
  Uniform

Path: torch.distributions.von_mises
  VonMises

Path: torch.distributions.weibull
  Weibull

Path: torch.fft
  fft
  fft2
  fftfreq
  fftn
  fftshift
  hfft
  ifft
  ifft2
  ifftn
  ifftshift
  ihfft
  irfft
  irfft2
  irfftn
  rfft
  rfft2
  rfftfreq
  rfftn

Path: torch.futures
  Future
  collect_all
  wait_all

Path: torch.fx
  Graph
  GraphModule
  Interpreter
  Node
  Proxy
  Tracer
  Transformer
  replace_pattern
  symbolic_trace
  wrap

Path: torch.hub
  download_url_to_file
  get_dir
  help
  list
  load
  load_state_dict_from_url
  set_dir

Path: torch.jit
  ScriptFunction
  ScriptModule
  annotate
  fork
  freeze
  ignore
  isinstance
  load
  optimize_for_inference
  save
  script
  script_if_tracing
  trace
  trace_module
  unused
  wait

Path: torch.linalg
  cholesky
  cholesky_ex
  cond
  det
  eig
  eigh
  eigvals
  eigvalsh
  householder_product
  inv
  inv_ex
  lstsq
  matmul
  matrix_norm
  matrix_power
  matrix_rank
  multi_dot
  norm
  pinv
  qr
  slogdet
  solve
  svd
  svdvals
  tensorinv
  tensorsolve
  vector_norm

Path: torch.nn
  AdaptiveAvgPool1d
  AdaptiveAvgPool2d
  AdaptiveAvgPool3d
  AdaptiveLogSoftmaxWithLoss
  AdaptiveMaxPool1d
  AdaptiveMaxPool2d
  AdaptiveMaxPool3d
  AlphaDropout
  AvgPool1d
  AvgPool2d
  AvgPool3d
  BCELoss
  BCEWithLogitsLoss
  BatchNorm1d
  BatchNorm2d
  BatchNorm3d
  Bilinear
  CELU
  CTCLoss
  ChannelShuffle
  ConstantPad1d
  ConstantPad2d
  ConstantPad3d
  Conv1d
  Conv2d
  Conv3d
  ConvTranspose1d
  ConvTranspose2d
  ConvTranspose3d
  CosineEmbeddingLoss
  CosineSimilarity
  CrossEntropyLoss
  DataParallel
  Dropout
  Dropout2d
  Dropout3d
  ELU
  Embedding
  EmbeddingBag
  FeatureAlphaDropout
  Flatten
  Fold
  FractionalMaxPool2d
  FractionalMaxPool3d
  GELU
  GLU
  GRU
  GRUCell
  GaussianNLLLoss
  GroupNorm
  Hardshrink
  Hardsigmoid
  Hardswish
  Hardtanh
  HingeEmbeddingLoss
  HuberLoss
  Identity
  InstanceNorm1d
  InstanceNorm2d
  InstanceNorm3d
  KLDivLoss
  L1Loss
  LPPool1d
  LPPool2d
  LSTM
  LSTMCell
  LayerNorm
  LazyBatchNorm1d
  LazyBatchNorm2d
  LazyBatchNorm3d
  LazyConv1d
  LazyConv2d
  LazyConv3d
  LazyConvTranspose1d
  LazyConvTranspose2d
  LazyConvTranspose3d
  LazyInstanceNorm1d
  LazyInstanceNorm2d
  LazyInstanceNorm3d
  LazyLinear
  LeakyReLU
  Linear
  LocalResponseNorm
  LogSigmoid
  LogSoftmax
  MSELoss
  MarginRankingLoss
  MaxPool1d
  MaxPool2d
  MaxPool3d
  MaxUnpool1d
  MaxUnpool2d
  MaxUnpool3d
  Mish
  Module
  ModuleDict
  ModuleList
  MultiLabelMarginLoss
  MultiLabelSoftMarginLoss
  MultiMarginLoss
  MultiheadAttention
  NLLLoss
  PReLU
  PairwiseDistance
  ParameterDict
  ParameterList
  PixelShuffle
  PixelUnshuffle
  PoissonNLLLoss
  RNN
  RNNBase
  RNNCell
  RReLU
  ReLU
  ReLU6
  ReflectionPad1d
  ReflectionPad2d
  ReflectionPad3d
  ReplicationPad1d
  ReplicationPad2d
  ReplicationPad3d
  SELU
  Sequential
  SiLU
  Sigmoid
  SmoothL1Loss
  SoftMarginLoss
  Softmax
  Softmax2d
  Softmin
  Softplus
  Softshrink
  Softsign
  SyncBatchNorm
  Tanh
  Tanhshrink
  Threshold
  Transformer
  TransformerDecoder
  TransformerDecoderLayer
  TransformerEncoder
  TransformerEncoderLayer
  TripletMarginLoss
  TripletMarginWithDistanceLoss
  Unflatten
  Unfold
  Upsample
  UpsamplingBilinear2d
  UpsamplingNearest2d
  ZeroPad2d

Path: torch.nn.functional
  adaptive_avg_pool1d
  adaptive_avg_pool2d
  adaptive_avg_pool3d
  adaptive_max_pool1d
  adaptive_max_pool2d
  adaptive_max_pool3d
  affine_grid
  alpha_dropout
  avg_pool1d
  avg_pool2d
  avg_pool3d
  batch_norm
  bilinear
  binary_cross_entropy
  binary_cross_entropy_with_logits
  celu
  conv1d
  conv2d
  conv3d
  conv_transpose1d
  conv_transpose2d
  conv_transpose3d
  cosine_embedding_loss
  cosine_similarity
  cross_entropy
  ctc_loss
  dropout
  dropout2d
  dropout3d
  elu
  elu_
  embedding
  embedding_bag
  feature_alpha_dropout
  fold
  fractional_max_pool2d
  fractional_max_pool3d
  gaussian_nll_loss
  gelu
  glu
  grid_sample
  group_norm
  gumbel_softmax
  hardshrink
  hardsigmoid
  hardswish
  hardtanh
  hardtanh_
  hinge_embedding_loss
  huber_loss
  instance_norm
  interpolate
  kl_div
  l1_loss
  layer_norm
  leaky_relu
  leaky_relu_
  linear
  local_response_norm
  log_softmax
  logsigmoid
  lp_pool1d
  lp_pool2d
  margin_ranking_loss
  max_pool1d
  max_pool2d
  max_pool3d
  max_unpool1d
  max_unpool2d
  max_unpool3d
  mish
  mse_loss
  multi_margin_loss
  multilabel_margin_loss
  multilabel_soft_margin_loss
  nll_loss
  normalize
  one_hot
  pad
  pairwise_distance
  pdist
  pixel_shuffle
  pixel_unshuffle
  poisson_nll_loss
  prelu
  relu
  relu6
  relu_
  rrelu
  rrelu_
  selu
  sigmoid
  silu
  smooth_l1_loss
  soft_margin_loss
  softmax
  softmin
  softplus
  softshrink
  softsign
  tanh
  tanhshrink
  threshold
  threshold_
  triplet_margin_loss
  triplet_margin_with_distance_loss
  unfold
  upsample
  upsample_bilinear
  upsample_nearest

Path: torch.nn.init
  calculate_gain
  constant_
  dirac_
  eye_
  kaiming_normal_
  kaiming_uniform_
  normal_
  ones_
  orthogonal_
  sparse_
  uniform_
  xavier_normal_
  xavier_uniform_
  zeros_

Path: torch.nn.modules.lazy
  LazyModuleMixin

Path: torch.nn.modules.module
  register_module_backward_hook
  register_module_forward_hook
  register_module_forward_pre_hook
  register_module_full_backward_hook

Path: torch.nn.parallel
  DistributedDataParallel

Path: torch.nn.parameter
  Parameter
  UninitializedBuffer
  UninitializedParameter

Path: torch.nn.utils
  clip_grad_norm_
  clip_grad_value_
  parameters_to_vector
  remove_spectral_norm
  remove_weight_norm
  skip_init
  spectral_norm
  vector_to_parameters
  weight_norm

Path: torch.nn.utils.parametrizations
  orthogonal
  spectral_norm

Path: torch.nn.utils.parametrize
  ParametrizationList
  cached
  is_parametrized
  register_parametrization
  remove_parametrizations

Path: torch.nn.utils.prune
  BasePruningMethod
  CustomFromMask
  Identity
  L1Unstructured
  LnStructured
  PruningContainer
  RandomStructured
  RandomUnstructured
  custom_from_mask
  global_unstructured
  is_pruned
  l1_unstructured
  ln_structured
  random_structured
  random_unstructured
  remove

Path: torch.nn.utils.rnn
  PackedSequence
  pack_padded_sequence
  pack_sequence
  pad_packed_sequence
  pad_sequence

Path: torch.onnx
  export
  export_to_pretty_string
  is_in_onnx_export
  register_custom_op_symbolic
  select_model_mode_for_export

Path: torch.optim
  ASGD
  Adadelta
  Adagrad
  Adam
  AdamW
  Adamax
  LBFGS
  NAdam
  Optimizer
  RAdam
  RMSprop
  Rprop
  SGD
  SparseAdam

Path: torch.optim.lr_scheduler
  ChainedScheduler
  ConstantLR
  CosineAnnealingLR
  CosineAnnealingWarmRestarts
  CyclicLR
  ExponentialLR
  LambdaLR
  LinearLR
  MultiStepLR
  MultiplicativeLR
  OneCycleLR
  ReduceLROnPlateau
  SequentialLR
  StepLR

Path: torch.overrides
  get_ignored_functions
  get_overridable_functions
  get_testing_overrides
  handle_torch_function
  has_torch_function
  is_tensor_like
  is_tensor_method_or_property
  wrap_torch_function

Path: torch.package
  Directory
  EmptyMatchError
  PackageExporter
  PackageImporter
  PackagingError

Path: torch.profiler
  ProfilerAction
  ProfilerActivity
  profile
  schedule
  tensorboard_trace_handler

Path: torch.quasirandom
  SobolEngine

Path: torch.random
  fork_rng
  get_rng_state
  initial_seed
  manual_seed
  seed
  set_rng_state

Path: torch.sparse
  addmm
  log_softmax
  mm
  softmax
  sum

Path: torch.special
  digamma
  entr
  erf
  erfc
  erfcx
  erfinv
  exp2
  expit
  expm1
  gammainc
  gammaincc
  gammaln
  i0
  i0e
  i1
  i1e
  log1p
  log_softmax
  logit
  logsumexp
  multigammaln
  ndtr
  ndtri
  polygamma
  psi
  round
  sinc
  xlog1py
  xlogy
  zeta

Path: torch.testing
  assert_close
  make_tensor

Path: torch.utils.benchmark
  CallgrindStats
  FunctionCounts
  Measurement
  Timer

Path: torch.utils.checkpoint
  checkpoint
  checkpoint_sequential

Path: torch.utils.cpp_extension
  BuildExtension
  CUDAExtension
  CppExtension
  check_compiler_abi_compatibility
  include_paths
  is_ninja_available
  load
  load_inline
  verify_ninja_availability

Path: torch.utils.data
  BatchSampler
  ChainDataset
  ConcatDataset
  DataLoader
  Dataset
  IterableDataset
  RandomSampler
  Sampler
  SequentialSampler
  Subset
  SubsetRandomSampler
  TensorDataset
  WeightedRandomSampler
  get_worker_info
  random_split

Path: torch.utils.data.distributed
  DistributedSampler

Path: torch.utils.dlpack
  from_dlpack
  to_dlpack

Path: torch.utils.mobile_optimizer
  optimize_for_mobile

Path: torch.utils.model_zoo
  load_url

Path: torch.utils.tensorboard.writer
  SummaryWriter

Path: torctorch.repeat_interleaveh
  count_nonzero

